{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BARCLAYS OPTION TRADING STRATEGY'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''BARCLAYS OPTION TRADING STRATEGY'''\n",
    "#This strat aims to exploit the gap between implied option vol and historical vol (volatility risk premium)\n",
    "#Retail traders in the past years have created huge liquidity for short-dated calls on large cap stocks\n",
    "#We build a strat around this that basically buys undervalued calls and sells overvalued ones\n",
    "#The expiration date we select is going to be 2 weeks, as this is where most liquidiy is concentrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import datetime\n",
    "import requests\n",
    "from pymongo import MongoClient\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "ALPHA_API = os.getenv(\"ALPHA_VANTAGE\")\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client['Barclays-Options']\n",
    "collection = db['options-data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  503 of 503 completed\n"
     ]
    }
   ],
   "source": [
    "'''Taking data from spy stocks to build the strat around'''\n",
    "#We parse the wikipedia page for spy data\n",
    "\n",
    "sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "sp500['Symbol'] = sp500['Symbol'].str.replace('.', '-')\n",
    "tickers = sp500['Symbol'].unique().tolist()\n",
    "\n",
    "data = yf.download(tickers, period='1y')\n",
    "data = data.stack()\n",
    "data.index.names = ['date', 'ticker']\n",
    "data.columns = data.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Well now filter by dollar volume to only opearte with the stocks which fit our liquidity constraints'''\n",
    "#We will compute the mean dollar volume from the past year to filter liquid stocks\n",
    "#Further, we selecct large cap stocks, as retail trader liquidity is more likely concentrated here\n",
    "\n",
    "latest_date = data.index.get_level_values('date').max() #we use this below for option vol\n",
    "\n",
    "data['dollarvol'] = (data['close'] * data['volume']) / 1e6\n",
    "data = data.dfby('ticker').mean() #finding mean dollar vol over past year\n",
    "\n",
    "data = data.sort_values(by='dollarvol', ascending = False)\n",
    "data = data.head(20) #top 20 liquid stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker  1  done\n",
      "Ticker  2  done\n",
      "Ticker  3  done\n",
      "Ticker  4  done\n",
      "Ticker  5  done\n",
      "Ticker  6  done\n",
      "Ticker  7  done\n",
      "Ticker  8  done\n",
      "Ticker  9  done\n",
      "Ticker  10  done\n",
      "Ticker  11  done\n",
      "Ticker  12  done\n",
      "Ticker  13  done\n",
      "Ticker  14  done\n",
      "Ticker  15  done\n",
      "Ticker  16  done\n",
      "Ticker  17  done\n",
      "Ticker  18  done\n",
      "Ticker  19  done\n",
      "Ticker  20  done\n"
     ]
    }
   ],
   "source": [
    "'''Lets try to build a options volume column in the dataframe'''\n",
    "#I tweaked with the weeks value and found the most liquidity to be in 2 weeks expriration date options, so we go with that\n",
    "#This is an incredibly slow process, as yfinance doesn't give option volume directly and we have to find a workaround\n",
    "#We now filter top 10 stocks with highest option volume\n",
    "#Liquidity is either most concentrated in 1 week expiry options or 2 week expiry options\n",
    "\n",
    "'''BUG => Too slow when trying to first filter by option volume, so did that later'''\n",
    "\n",
    "data['option_volume'] = None\n",
    "data['sector'] = None\n",
    "\n",
    "latest_tickers = data.index.get_level_values('ticker')\n",
    "\n",
    "count = 1\n",
    "\n",
    "for tckr in latest_tickers:\n",
    "    #expiration_date = (latest_date + timedelta(weeks=2)).strftime('%Y-%m-%d') #or tckr.options[1]\n",
    "\n",
    "    tk = yf.Ticker(tckr)\n",
    "    expiration_date = tk.options[1]\n",
    "    opt_chain = tk.option_chain(expiration_date)\n",
    "\n",
    "    option_volume = opt_chain.calls['volume'].sum() + opt_chain.puts['volume'].sum()\n",
    "\n",
    "    data.loc[tckr, 'option_volume'] = option_volume\n",
    "    data.loc[tckr, 'sector'] = tk.info.get('sector', 'NA')\n",
    "\n",
    "    print(\"Ticker \", count, \" done\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Checking how different max_dollar_volume and max_option_volume tickers are'''\n",
    "\n",
    "data = data.sort_values(by='dollarvol', ascending = False)\n",
    "max_dollar_vol = data.head(10)\n",
    "\n",
    "data = data.sort_values(by='option_volume', ascending = False)\n",
    "max_option_vol = data.head(10)\n",
    "\n",
    "max_dvol= set(max_dollar_vol.index.to_list())\n",
    "max_ovol = set(max_option_vol.index.to_list())\n",
    "\n",
    "max_dvol.difference(max_ovol)\n",
    "max_ovol.difference(max_dvol)\n",
    "\n",
    "data = max_option_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now, we will create a hashmap that maps each sector to its exchange traded fund(if it exists)'''\n",
    "#We will need this later for the stat arb strategy\n",
    "#There is no easy way to do this, so well select all popular etfs, view the sector of the top holding of each and create a map\n",
    "\n",
    "etfs = [\n",
    "    'XLK',  # Technology Select Sector SPDR Fund\n",
    "    'XLF',  # Financial Select Sector SPDR Fund\n",
    "    'XLV',  # Health Care Select Sector SPDR Fund\n",
    "    'XLY',  # Consumer Discretionary Select Sector SPDR Fund\n",
    "    'XLP',  # Consumer Staples Select Sector SPDR Fund\n",
    "    'XLE',  # Energy Select Sector SPDR Fund\n",
    "    'XLB',  # Materials Select Sector SPDR Fund\n",
    "    'XLI',  # Industrial Select Sector SPDR Fund\n",
    "    'XLU',  # Utilities Select Sector SPDR Fund\n",
    "    'XLRE', # Real Estate Select Sector SPDR Fund\n",
    "    'XLC',  # Communication Services Select Sector SPDR Fund\n",
    "    'EEM',  # iShares MSCI Emerging Markets ETF\n",
    "    'SPY',  # SPDR S&P 500 ETF Trust\n",
    "    'VTI',  # Vanguard Total Stock Market ETF\n",
    "    'VTV'   # Vanguard Value ETF\n",
    "]\n",
    "\n",
    "sector_to_etf = {}\n",
    "\n",
    "for etf in etfs:\n",
    "    etf_data = yf.Ticker(etf).funds_data.top_holdings\n",
    "    etf_ticker = etf_data.index[0] #first holding\n",
    "    sector = yf.Ticker(etf_ticker).info.get('sector', 'NA')\n",
    "    sector_to_etf[sector] = etf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CONSIDERATIONS'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''NOW COMES THE FUN QUANT PART'''\n",
    "#We need a VolScore metric that can identify the spread of volatility risk premium\n",
    "#Barclays doesn't reveal what they used to calculate volScore\n",
    "#An educated guess (based on traditional pairs trading) is that we compare implied vol to historical vol AND sector vol\n",
    "#Underlying assumption is that vol deviations are temporary (mean reversion)\n",
    "#The formula I agree on is:\n",
    "\n",
    "#            IV - (w1 * HV  +  w2 * SV)\n",
    "# VolScore = --------------------------\n",
    "#                    sigma_res\n",
    "\n",
    "#w1 and w2 are weights that we'll determine with RollingOLS or Kalman Filter\n",
    "#Weights should be time-varying preferably\n",
    "#There is no need to weight IV too as the rationale is that IV already embeds the risk premium\n",
    "#This comes from training GARCH models research - Univ of North Carolina research\n",
    "\n",
    "# IV = w0 + w1 * HV + w2 * SV + epsilon_t\n",
    "# epsilon_t is our error term, IV - IV_hat or residual\n",
    "# IV is actual implied volatility and IV_hat is what our model estimates it to be\n",
    "# The error term in the reason we normalize as we can't say if a given residue is significant or just market noise\n",
    "\n",
    "#sigma_res is the standard deviation of residuals\n",
    "#historical deviation of IV from weighted avg of HV and SV\n",
    "#This normalization lets us use VolScore like a z-score, which lets us better gauge tradding opps\n",
    "\n",
    "'''CONSIDERATIONS'''\n",
    "#Use HFT data for HV as it helps capture intraday price movements that daily data can miss\n",
    "#this reduces noise in data as shown by past research - Barndorff-Nielsen and Shephard\n",
    "#Might have to do more research on how many years of data is best suited for pairs trading, curr is 1 yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  3 of 3 completed\n"
     ]
    }
   ],
   "source": [
    "'''Downloading etf data'''\n",
    "#We will need this to compute the historical volatility of the sector\n",
    "#We'll compute the daily volatilites for every day, mean them and find the annualized avg vol\n",
    "\n",
    "\n",
    "req_etfs = [sector_to_etf[s] for s in data.sector.unique()]\n",
    "tickers = \" \".join(req_etfs)\n",
    "\n",
    "etf_data = yf.download(tickers, period=\"2y\")\n",
    "etf_data = etf_data.stack()\n",
    "etf_data.index.names = ['date', 'ticker']\n",
    "etf_data.columns = etf_data.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Computing historical volatility'''\n",
    "\n",
    "def calculate_hv(df, col):\n",
    "    df = df.sort_index(level='date')\n",
    "    \n",
    "    returns = df['close'] / df['close'].shift(1)\n",
    "    df['log_returns'] = np.log(returns)\n",
    "    rolling_vol = df['log_returns'].rolling(window=252).std()\n",
    "    df[col] = rolling_vol * np.sqrt(252)  # annual vol\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etff = yf.download('XLY', period='1mo')\n",
    "# etff = etff.sort_values(by = \"Date\", ascending=False)\n",
    "# etff = etff.head(2)\n",
    "# print(etff.Close.XLY[-1], etff.Close.XLY[0])\n",
    "# np.log(etff.Close.XLY[0] / etff.Close.XLY[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  10 of 10 completed\n"
     ]
    }
   ],
   "source": [
    "'''Now we download the data for the 10 stocks we selected'''\n",
    "#These contain all the actual option contracts we will be trading\n",
    "#The dataframe consists of a IV section, which we'll use in the model we created\n",
    "\n",
    "tickers = \" \".join(data.index.tolist())\n",
    "\n",
    "options_data = yf.download(tickers, period=\"2y\")\n",
    "options_data = options_data.stack()\n",
    "options_data.index.names = ['date', 'ticker']\n",
    "options_data.columns = options_data.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etf_data = etf_data.dfby(\"ticker\").apply(calculate_hv, 'HV').dropna()\n",
    "options_data = options_data.dfby(\"ticker\").apply(calculate_hv, 'RV').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Run this script only once, otherwise necessary cols will be deleted'''\n",
    "\n",
    "etf_data = etf_data.reset_index(level=2, drop=True) #removing duplicate ticker index\n",
    "options_data = options_data.reset_index(level=2, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Modifying the options_data dataframe to include the HV for the corresponding ticker-etf pair'''\n",
    "#BUG: for loop was very slow here, so i ended up using previous dataframes mapping\n",
    "\n",
    "options_data['sector'] = options_data.index.get_level_values('ticker').map(data['sector'])\n",
    "options_data['ETF'] = options_data['sector'].map(sector_to_etf)\n",
    "\n",
    "func = lambda row: etf_data.loc[(row['ETF'], row.name[1]), 'HV'] if (row['ETF'], row.name[1]) in etf_data.index else None\n",
    "\n",
    "options_data['SV'] = options_data.apply(func, axis=1)\n",
    "\n",
    "options_data.drop(columns=['ETF', 'sector'], axis=1, inplace=True)\n",
    "options_data = options_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Downloading options data for past two years to compute VIX'''\n",
    "#Im using alphavantage API as this was the best service that I could find for free\n",
    "#However alphavantage doesn't have a good way to directly fetch data as a dataframe\n",
    "#It gives responses in json, which Im manually converting into a pd Df\n",
    "#Because alphavantage rate limits (25 per day), ill store data to a MongoDB cluster first\n",
    "\n",
    "two_yrs = (datetime.datetime.today() - datetime.timedelta(weeks=104)).strftime('%Y-%m-%d')\n",
    "\n",
    "for ticker in data.index.get_level_values('ticker').tolist():\n",
    "    url = 'https://www.alphavantage.co/query?' \\\n",
    "    'function=HISTORICAL_OPTIONS&symbol='+ticker+'&apikey='+ALPHA_API+'&date='+two_yrs\n",
    "\n",
    "    r = requests.get(url)\n",
    "\n",
    "    push_data = r.json()\n",
    "    push_data['_id'] = ticker\n",
    "\n",
    "    collection.delete_one({\"_id\": ticker})\n",
    "    collection.insert_one(push_data)\n",
    "\n",
    "    print(\"Data pushed to cluster for \" + ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Script to convert json data to Pandas dataframe'''\n",
    "'''Script to query for data of a specific ticker from our Mongo cluster'''\n",
    "\n",
    "def json_to_df(json_data): #json_data should be r.json()['data']\n",
    "    ticker_options = pd.DataFrame()\n",
    "    for day_data in json_data:\n",
    "        ticker_options = pd.concat([ticker_options, pd.DataFrame([day_data])], ignore_index=True)\n",
    "\n",
    "    return ticker_options\n",
    "\n",
    "def get_data(ticker):\n",
    "    mongo_data = collection.find_one({\"_id\": ticker})\n",
    "    if mongo_data is None:\n",
    "        return \"Data not found for \" + ticker\n",
    "    else:\n",
    "        return json_to_df(mongo_data['data']).sort_values(by='expiration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Now we need to compute the implied volatitlies of each ticker'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Now we need to compute the implied volatitlies of each ticker'''\n",
    "#We run into a problem where each option has option contracts with differing volatilities\n",
    "#A simple average wont cut it as options differ in ATM and ITM status, and in moneyness\n",
    "#A weighted average is a good place to start\n",
    "#We can also build our own VIX like model\n",
    "\n",
    "# The formula is: \n",
    "#    sigma² = (2 * e^(r*T) / T) * Σ [ (ΔK_i / K_i²) * Q(K_i) ] - (1/T) * ((F/K₀ - 1)²)\n",
    "\n",
    "# T is the time to expiration in years (typically 30/365 for 30 days), and r is the risk-free rate.\n",
    "# ΔK_i represents the interval between adjacent strikes, K_i are the strike prices, and Q(K_i) are the option mid-prices.\n",
    "# F is the forward price of the underlying asset, and K₀ is the first strike below F.\n",
    "# Taking the square root of sigma² gives the implied volatility (IV), in the same annualized units as HV and RV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Computing IV for each row in options data'''\n",
    "#Well use the above formula with a 30 day rolling window and the option data in our DB to do this\n",
    "\n",
    "#BUG => The data quality seems to be very poor as there are unrealistic striker for TSLA ($1.67)\n",
    "#So, ill clean the data by going through each needed column and dropping outliers\n",
    "#however i also need to ensure that my model works for tail events, and properly need to only remove bad data, while still keeping the tail event data\n",
    "\n",
    "#perhaps its just best to use weighted average with open interest as weights, the free data seems too poor quality\n",
    "\n",
    "def clean_data(df, cols, lower=0.01, upper=0.99):\n",
    "    for c in ['implied_volatility', 'volume', 'open_interest']:\n",
    "        df[c] = pd.to_numeric(df[c])\n",
    "\n",
    "    for col in cols:\n",
    "        # Convert col vals to numeric coercing errors to NaN\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        # drop rows where the conversion failed (NaN values)\n",
    "        df = df[df[col].notna()]\n",
    "        \n",
    "        l = df[col].quantile(lower)\n",
    "        u = df[col].quantile(upper)\n",
    "        df = df[(df[col] >= l) & (df[col] <= u)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Computing weighted averages for each ticker and appending that to our options_data'''\n",
    "#The weighted average formula we use weights both the open interest and volume of traded options\n",
    "\n",
    "raw_data = get_data('TSLA')\n",
    "cleaned_data = clean_data(raw_data, ['strike', 'bid', 'ask'])\n",
    "weighted_avg = lambda df: pd.Series(\n",
    "        (df['implied_volatility'] * (df['volume'] + df['open_interest'])).sum() / \n",
    "        (df['volume'] + df['open_interest']).sum(), \n",
    "        index=df.index)\n",
    "\n",
    "cleaned_data['IV'] = cleaned_data.groupby('expiration').apply(weighted_avg).reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.40906599, 0.68194464, 0.65495531, 0.6734333 , 0.66283482,\n",
       "       0.79598515, 0.66258321, 0.6902437 , 0.73587393, 0.67371818,\n",
       "       0.64138158, 0.64427761, 0.60399838, 0.61399547, 0.62172202,\n",
       "       0.6246212 , 0.61569544, 0.60570824, 0.59872351, 0.6143326 ,\n",
       "       0.61402866])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.IV.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2024-03-13 00:00:00'),\n",
       " Timestamp('2024-03-14 00:00:00'),\n",
       " Timestamp('2024-03-15 00:00:00'),\n",
       " Timestamp('2024-03-18 00:00:00'),\n",
       " Timestamp('2024-03-19 00:00:00'),\n",
       " Timestamp('2024-03-20 00:00:00'),\n",
       " Timestamp('2024-03-21 00:00:00'),\n",
       " Timestamp('2024-03-22 00:00:00'),\n",
       " Timestamp('2024-03-25 00:00:00'),\n",
       " Timestamp('2024-03-26 00:00:00'),\n",
       " Timestamp('2024-03-27 00:00:00'),\n",
       " Timestamp('2024-03-28 00:00:00'),\n",
       " Timestamp('2024-04-01 00:00:00'),\n",
       " Timestamp('2024-04-02 00:00:00'),\n",
       " Timestamp('2024-04-03 00:00:00'),\n",
       " Timestamp('2024-04-04 00:00:00'),\n",
       " Timestamp('2024-04-05 00:00:00'),\n",
       " Timestamp('2024-04-08 00:00:00'),\n",
       " Timestamp('2024-04-09 00:00:00'),\n",
       " Timestamp('2024-04-10 00:00:00'),\n",
       " Timestamp('2024-04-11 00:00:00'),\n",
       " Timestamp('2024-04-12 00:00:00'),\n",
       " Timestamp('2024-04-15 00:00:00'),\n",
       " Timestamp('2024-04-16 00:00:00'),\n",
       " Timestamp('2024-04-17 00:00:00'),\n",
       " Timestamp('2024-04-18 00:00:00'),\n",
       " Timestamp('2024-04-19 00:00:00'),\n",
       " Timestamp('2024-04-22 00:00:00'),\n",
       " Timestamp('2024-04-23 00:00:00'),\n",
       " Timestamp('2024-04-24 00:00:00'),\n",
       " Timestamp('2024-04-25 00:00:00'),\n",
       " Timestamp('2024-04-26 00:00:00'),\n",
       " Timestamp('2024-04-29 00:00:00'),\n",
       " Timestamp('2024-04-30 00:00:00'),\n",
       " Timestamp('2024-05-01 00:00:00'),\n",
       " Timestamp('2024-05-02 00:00:00'),\n",
       " Timestamp('2024-05-03 00:00:00'),\n",
       " Timestamp('2024-05-06 00:00:00'),\n",
       " Timestamp('2024-05-07 00:00:00'),\n",
       " Timestamp('2024-05-08 00:00:00'),\n",
       " Timestamp('2024-05-09 00:00:00'),\n",
       " Timestamp('2024-05-10 00:00:00'),\n",
       " Timestamp('2024-05-13 00:00:00'),\n",
       " Timestamp('2024-05-14 00:00:00'),\n",
       " Timestamp('2024-05-15 00:00:00'),\n",
       " Timestamp('2024-05-16 00:00:00'),\n",
       " Timestamp('2024-05-17 00:00:00'),\n",
       " Timestamp('2024-05-20 00:00:00'),\n",
       " Timestamp('2024-05-21 00:00:00'),\n",
       " Timestamp('2024-05-22 00:00:00'),\n",
       " Timestamp('2024-05-23 00:00:00'),\n",
       " Timestamp('2024-05-24 00:00:00'),\n",
       " Timestamp('2024-05-28 00:00:00'),\n",
       " Timestamp('2024-05-29 00:00:00'),\n",
       " Timestamp('2024-05-30 00:00:00'),\n",
       " Timestamp('2024-05-31 00:00:00'),\n",
       " Timestamp('2024-06-03 00:00:00'),\n",
       " Timestamp('2024-06-04 00:00:00'),\n",
       " Timestamp('2024-06-05 00:00:00'),\n",
       " Timestamp('2024-06-06 00:00:00'),\n",
       " Timestamp('2024-06-07 00:00:00'),\n",
       " Timestamp('2024-06-10 00:00:00'),\n",
       " Timestamp('2024-06-11 00:00:00'),\n",
       " Timestamp('2024-06-12 00:00:00'),\n",
       " Timestamp('2024-06-13 00:00:00'),\n",
       " Timestamp('2024-06-14 00:00:00'),\n",
       " Timestamp('2024-06-17 00:00:00'),\n",
       " Timestamp('2024-06-18 00:00:00'),\n",
       " Timestamp('2024-06-20 00:00:00'),\n",
       " Timestamp('2024-06-21 00:00:00'),\n",
       " Timestamp('2024-06-24 00:00:00'),\n",
       " Timestamp('2024-06-25 00:00:00'),\n",
       " Timestamp('2024-06-26 00:00:00'),\n",
       " Timestamp('2024-06-27 00:00:00'),\n",
       " Timestamp('2024-06-28 00:00:00'),\n",
       " Timestamp('2024-07-01 00:00:00'),\n",
       " Timestamp('2024-07-02 00:00:00'),\n",
       " Timestamp('2024-07-03 00:00:00'),\n",
       " Timestamp('2024-07-05 00:00:00'),\n",
       " Timestamp('2024-07-08 00:00:00'),\n",
       " Timestamp('2024-07-09 00:00:00'),\n",
       " Timestamp('2024-07-10 00:00:00'),\n",
       " Timestamp('2024-07-11 00:00:00'),\n",
       " Timestamp('2024-07-12 00:00:00'),\n",
       " Timestamp('2024-07-15 00:00:00'),\n",
       " Timestamp('2024-07-16 00:00:00'),\n",
       " Timestamp('2024-07-17 00:00:00'),\n",
       " Timestamp('2024-07-18 00:00:00'),\n",
       " Timestamp('2024-07-19 00:00:00'),\n",
       " Timestamp('2024-07-22 00:00:00'),\n",
       " Timestamp('2024-07-23 00:00:00'),\n",
       " Timestamp('2024-07-24 00:00:00'),\n",
       " Timestamp('2024-07-25 00:00:00'),\n",
       " Timestamp('2024-07-26 00:00:00'),\n",
       " Timestamp('2024-07-29 00:00:00'),\n",
       " Timestamp('2024-07-30 00:00:00'),\n",
       " Timestamp('2024-07-31 00:00:00'),\n",
       " Timestamp('2024-08-01 00:00:00'),\n",
       " Timestamp('2024-08-02 00:00:00'),\n",
       " Timestamp('2024-08-05 00:00:00'),\n",
       " Timestamp('2024-08-06 00:00:00'),\n",
       " Timestamp('2024-08-07 00:00:00'),\n",
       " Timestamp('2024-08-08 00:00:00'),\n",
       " Timestamp('2024-08-09 00:00:00'),\n",
       " Timestamp('2024-08-12 00:00:00'),\n",
       " Timestamp('2024-08-13 00:00:00'),\n",
       " Timestamp('2024-08-14 00:00:00'),\n",
       " Timestamp('2024-08-15 00:00:00'),\n",
       " Timestamp('2024-08-16 00:00:00'),\n",
       " Timestamp('2024-08-19 00:00:00'),\n",
       " Timestamp('2024-08-20 00:00:00'),\n",
       " Timestamp('2024-08-21 00:00:00'),\n",
       " Timestamp('2024-08-22 00:00:00'),\n",
       " Timestamp('2024-08-23 00:00:00'),\n",
       " Timestamp('2024-08-26 00:00:00'),\n",
       " Timestamp('2024-08-27 00:00:00'),\n",
       " Timestamp('2024-08-28 00:00:00'),\n",
       " Timestamp('2024-08-29 00:00:00'),\n",
       " Timestamp('2024-08-30 00:00:00'),\n",
       " Timestamp('2024-09-03 00:00:00'),\n",
       " Timestamp('2024-09-04 00:00:00'),\n",
       " Timestamp('2024-09-05 00:00:00'),\n",
       " Timestamp('2024-09-06 00:00:00'),\n",
       " Timestamp('2024-09-09 00:00:00'),\n",
       " Timestamp('2024-09-10 00:00:00'),\n",
       " Timestamp('2024-09-11 00:00:00'),\n",
       " Timestamp('2024-09-12 00:00:00'),\n",
       " Timestamp('2024-09-13 00:00:00'),\n",
       " Timestamp('2024-09-16 00:00:00'),\n",
       " Timestamp('2024-09-17 00:00:00'),\n",
       " Timestamp('2024-09-18 00:00:00'),\n",
       " Timestamp('2024-09-19 00:00:00'),\n",
       " Timestamp('2024-09-20 00:00:00'),\n",
       " Timestamp('2024-09-23 00:00:00'),\n",
       " Timestamp('2024-09-24 00:00:00'),\n",
       " Timestamp('2024-09-25 00:00:00'),\n",
       " Timestamp('2024-09-26 00:00:00'),\n",
       " Timestamp('2024-09-27 00:00:00'),\n",
       " Timestamp('2024-09-30 00:00:00'),\n",
       " Timestamp('2024-10-01 00:00:00'),\n",
       " Timestamp('2024-10-02 00:00:00'),\n",
       " Timestamp('2024-10-03 00:00:00'),\n",
       " Timestamp('2024-10-04 00:00:00'),\n",
       " Timestamp('2024-10-07 00:00:00'),\n",
       " Timestamp('2024-10-08 00:00:00'),\n",
       " Timestamp('2024-10-09 00:00:00'),\n",
       " Timestamp('2024-10-10 00:00:00'),\n",
       " Timestamp('2024-10-11 00:00:00'),\n",
       " Timestamp('2024-10-14 00:00:00'),\n",
       " Timestamp('2024-10-15 00:00:00'),\n",
       " Timestamp('2024-10-16 00:00:00'),\n",
       " Timestamp('2024-10-17 00:00:00'),\n",
       " Timestamp('2024-10-18 00:00:00'),\n",
       " Timestamp('2024-10-21 00:00:00'),\n",
       " Timestamp('2024-10-22 00:00:00'),\n",
       " Timestamp('2024-10-23 00:00:00'),\n",
       " Timestamp('2024-10-24 00:00:00'),\n",
       " Timestamp('2024-10-25 00:00:00'),\n",
       " Timestamp('2024-10-28 00:00:00'),\n",
       " Timestamp('2024-10-29 00:00:00'),\n",
       " Timestamp('2024-10-30 00:00:00'),\n",
       " Timestamp('2024-10-31 00:00:00'),\n",
       " Timestamp('2024-11-01 00:00:00'),\n",
       " Timestamp('2024-11-04 00:00:00'),\n",
       " Timestamp('2024-11-05 00:00:00'),\n",
       " Timestamp('2024-11-06 00:00:00'),\n",
       " Timestamp('2024-11-07 00:00:00'),\n",
       " Timestamp('2024-11-08 00:00:00'),\n",
       " Timestamp('2024-11-11 00:00:00'),\n",
       " Timestamp('2024-11-12 00:00:00'),\n",
       " Timestamp('2024-11-13 00:00:00'),\n",
       " Timestamp('2024-11-14 00:00:00'),\n",
       " Timestamp('2024-11-15 00:00:00'),\n",
       " Timestamp('2024-11-18 00:00:00'),\n",
       " Timestamp('2024-11-19 00:00:00'),\n",
       " Timestamp('2024-11-20 00:00:00'),\n",
       " Timestamp('2024-11-21 00:00:00'),\n",
       " Timestamp('2024-11-22 00:00:00'),\n",
       " Timestamp('2024-11-25 00:00:00'),\n",
       " Timestamp('2024-11-26 00:00:00'),\n",
       " Timestamp('2024-11-27 00:00:00'),\n",
       " Timestamp('2024-11-29 00:00:00'),\n",
       " Timestamp('2024-12-02 00:00:00'),\n",
       " Timestamp('2024-12-03 00:00:00'),\n",
       " Timestamp('2024-12-04 00:00:00'),\n",
       " Timestamp('2024-12-05 00:00:00'),\n",
       " Timestamp('2024-12-06 00:00:00'),\n",
       " Timestamp('2024-12-09 00:00:00'),\n",
       " Timestamp('2024-12-10 00:00:00'),\n",
       " Timestamp('2024-12-11 00:00:00'),\n",
       " Timestamp('2024-12-12 00:00:00'),\n",
       " Timestamp('2024-12-13 00:00:00'),\n",
       " Timestamp('2024-12-16 00:00:00'),\n",
       " Timestamp('2024-12-17 00:00:00'),\n",
       " Timestamp('2024-12-18 00:00:00'),\n",
       " Timestamp('2024-12-19 00:00:00'),\n",
       " Timestamp('2024-12-20 00:00:00'),\n",
       " Timestamp('2024-12-23 00:00:00'),\n",
       " Timestamp('2024-12-24 00:00:00'),\n",
       " Timestamp('2024-12-26 00:00:00'),\n",
       " Timestamp('2024-12-27 00:00:00'),\n",
       " Timestamp('2024-12-30 00:00:00'),\n",
       " Timestamp('2024-12-31 00:00:00'),\n",
       " Timestamp('2025-01-02 00:00:00'),\n",
       " Timestamp('2025-01-03 00:00:00'),\n",
       " Timestamp('2025-01-06 00:00:00'),\n",
       " Timestamp('2025-01-07 00:00:00'),\n",
       " Timestamp('2025-01-08 00:00:00'),\n",
       " Timestamp('2025-01-10 00:00:00'),\n",
       " Timestamp('2025-01-13 00:00:00'),\n",
       " Timestamp('2025-01-14 00:00:00'),\n",
       " Timestamp('2025-01-15 00:00:00'),\n",
       " Timestamp('2025-01-16 00:00:00'),\n",
       " Timestamp('2025-01-17 00:00:00'),\n",
       " Timestamp('2025-01-21 00:00:00'),\n",
       " Timestamp('2025-01-22 00:00:00'),\n",
       " Timestamp('2025-01-23 00:00:00'),\n",
       " Timestamp('2025-01-24 00:00:00'),\n",
       " Timestamp('2025-01-27 00:00:00'),\n",
       " Timestamp('2025-01-28 00:00:00'),\n",
       " Timestamp('2025-01-29 00:00:00'),\n",
       " Timestamp('2025-01-30 00:00:00'),\n",
       " Timestamp('2025-01-31 00:00:00'),\n",
       " Timestamp('2025-02-03 00:00:00'),\n",
       " Timestamp('2025-02-04 00:00:00'),\n",
       " Timestamp('2025-02-05 00:00:00'),\n",
       " Timestamp('2025-02-06 00:00:00'),\n",
       " Timestamp('2025-02-07 00:00:00'),\n",
       " Timestamp('2025-02-10 00:00:00'),\n",
       " Timestamp('2025-02-11 00:00:00'),\n",
       " Timestamp('2025-02-12 00:00:00'),\n",
       " Timestamp('2025-02-13 00:00:00'),\n",
       " Timestamp('2025-02-14 00:00:00'),\n",
       " Timestamp('2025-02-18 00:00:00'),\n",
       " Timestamp('2025-02-19 00:00:00'),\n",
       " Timestamp('2025-02-20 00:00:00'),\n",
       " Timestamp('2025-02-21 00:00:00'),\n",
       " Timestamp('2025-02-24 00:00:00'),\n",
       " Timestamp('2025-02-25 00:00:00'),\n",
       " Timestamp('2025-02-26 00:00:00'),\n",
       " Timestamp('2025-02-27 00:00:00'),\n",
       " Timestamp('2025-02-28 00:00:00'),\n",
       " Timestamp('2025-03-03 00:00:00'),\n",
       " Timestamp('2025-03-04 00:00:00'),\n",
       " Timestamp('2025-03-05 00:00:00'),\n",
       " Timestamp('2025-03-06 00:00:00'),\n",
       " Timestamp('2025-03-07 00:00:00'),\n",
       " Timestamp('2025-03-10 00:00:00'),\n",
       " Timestamp('2025-03-11 00:00:00'),\n",
       " Timestamp('2025-03-12 00:00:00')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options_data.index.get_level_values('date').unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Building the linear regression model'''\n",
    "#Now we build the linear regression model for VolScore based on the formula above\n",
    "#we use HV from etf_data and RV from options_data for the weights\n",
    "\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def compute_volscore(df, window):\n",
    "    df = df.sort_index(level = 'date').copy()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
